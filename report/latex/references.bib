@Article{Auer2002,
author="Auer, Peter
and Cesa-Bianchi, Nicol{\`o}
and Fischer, Paul",
title="Finite-time Analysis of the Multiarmed Bandit Problem",
journal="Machine Learning",
year="2002",
month="May",
day="01",
volume="47",
number="2",
pages="235--256",
abstract="Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.",
issn="1573-0565",
doi="10.1023/A:1013689704352",
url="https://doi.org/10.1023/A:1013689704352"
}

@article{Auer:2003,
 author = {Auer, Peter and Cesa-Bianchi, Nicol\`{o} and Freund, Yoav and Schapire, Robert E.},
 title = {The Nonstochastic Multiarmed Bandit Problem},
 journal = {SIAM J. Comput.},
 issue_date = {2003},
 volume = {32},
 number = {1},
 month = jan,
 year = {2003},
 issn = {0097-5397},
 pages = {48--77},
 numpages = {30},
 url = {https://doi.org/10.1137/S0097539701398375},
 doi = {10.1137/S0097539701398375},
 acmid = {589365},
 publisher = {Society for Industrial and Applied Mathematics},
 address = {Philadelphia, PA, USA},
 keywords = {adversarial bandit problem, unknown matrix games},
} 

@Article{Kearns2002,
author="Kearns, Michael
and Mansour, Yishay
and Ng, Andrew Y.",
title="A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes",
journal="Machine Learning",
year="2002",
month="Nov",
day="01",
volume="49",
number="2",
pages="193--208",
abstract="A critical issue for the application of Markov decision processes (MDPs) to realistic problems is how the complexity of planning scales with the size of the MDP. In stochastic environments with very large or infinite state spaces, traditional planning and reinforcement learning algorithms may be inapplicable, since their running time typically grows linearly with the state space size in the worst case. In this paper we present a new algorithm that, given only a generative model (a natural and common type of simulator) for an arbitrary MDP, performs on-line, near-optimal planning with a per-state running time that has no dependence on the number of states. The running time is exponential in the horizon time (which depends only on the discount factor $\gamma$ and the desired degree of approximation to the optimal policy). Our algorithm thus provides a different complexity trade-off than classical algorithms such as value iteration---rather than scaling linearly in both horizon time and state space size, our running time trades an exponential dependence on the former in exchange for no dependence on the latter.",
issn="1573-0565",
doi="10.1023/A:1017932429737",
url="https://doi.org/10.1023/A:1017932429737"
}

@inproceedings{Kocsis2006,
 author = {Kocsis, Levente and Szepesv\'{a}ri, Csaba},
 title = {Bandit Based Monte-carlo Planning},
 booktitle = {Proceedings of the 17th European Conference on Machine Learning},
 series = {ECML'06},
 year = {2006},
 isbn = {3-540-45375-X, 978-3-540-45375-8},
 location = {Berlin, Germany},
 pages = {282--293},
 numpages = {12},
 url = {http://dx.doi.org/10.1007/11871842_29},
 doi = {10.1007/11871842_29},
 acmid = {2091633},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
}

@article{robbins1952,
author = "Robbins, Herbert",
fjournal = "Bulletin of the American Mathematical Society",
journal = "Bull. Amer. Math. Soc.",
month = "09",
number = "5",
pages = "527--535",
publisher = "American Mathematical Society",
title = "Some aspects of the sequential design of experiments",
url = "https://projecteuclid.org:443/euclid.bams/1183517370",
volume = "58",
year = "1952"
}

@article{Jaksch:2010:NRB:1756006.1859902,
 author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
 title = {Near-optimal Regret Bounds for Reinforcement Learning},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2010},
 volume = {11},
 month = aug,
 year = {2010},
 issn = {1532-4435},
 pages = {1563--1600},
 numpages = {38},
 url = {http://dl.acm.org/citation.cfm?id=1756006.1859902},
 acmid = {1859902},
 publisher = {JMLR.org},
} 

@article{regretAnalysis,
url = {http://dx.doi.org/10.1561/2200000024},
year = {2012},
volume = {5},
journal = {Foundations and Trends® in Machine Learning},
title = {Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems},
doi = {10.1561/2200000024},
issn = {1935-8237},
number = {1},
pages = {1-122},
author = {Sébastien Bubeck and Nicolò Cesa-Bianchi}
}