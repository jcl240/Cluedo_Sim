@Article{Auer2002,
author="Auer, Peter
and Cesa-Bianchi, Nicol{\`o}
and Fischer, Paul",
title="Finite-time Analysis of the Multiarmed Bandit Problem",
journal="Machine Learning",
year="2002",
month="May",
day="01",
volume="47",
number="2",
pages="235--256",
abstract="Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.",
issn="1573-0565",
doi="10.1023/A:1013689704352",
url="https://doi.org/10.1023/A:1013689704352"
}

@INPROCEEDINGS{Silver-veness,
    author = {David Silver and Joel Veness},
    title = {Monte-Carlo Planning in Large POMDPs},
    booktitle = {In Advances in Neural Information Processing Systems 23},
    year = {2010},
    pages = {2164--2172}
}

@article{Auer:2003,
 author = {Auer, Peter and Cesa-Bianchi, Nicol\`{o} and Freund, Yoav and Schapire, Robert E.},
 title = {The Nonstochastic Multiarmed Bandit Problem},
 journal = {SIAM J. Comput.},
 issue_date = {2003},
 volume = {32},
 number = {1},
 month = jan,
 year = {2003},
 issn = {0097-5397},
 pages = {48--77},
 numpages = {30},
 url = {https://doi.org/10.1137/S0097539701398375},
 doi = {10.1137/S0097539701398375},
 acmid = {589365},
 publisher = {Society for Industrial and Applied Mathematics},
 address = {Philadelphia, PA, USA},
 keywords = {adversarial bandit problem, unknown matrix games},
} 

@Article{Kearns2002,
author="Kearns, Michael
and Mansour, Yishay
and Ng, Andrew Y.",
title="A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes",
journal="Machine Learning",
year="2002",
month="Nov",
day="01",
volume="49",
number="2",
pages="193--208",
abstract="A critical issue for the application of Markov decision processes (MDPs) to realistic problems is how the complexity of planning scales with the size of the MDP. In stochastic environments with very large or infinite state spaces, traditional planning and reinforcement learning algorithms may be inapplicable, since their running time typically grows linearly with the state space size in the worst case. In this paper we present a new algorithm that, given only a generative model (a natural and common type of simulator) for an arbitrary MDP, performs on-line, near-optimal planning with a per-state running time that has no dependence on the number of states. The running time is exponential in the horizon time (which depends only on the discount factor $\gamma$ and the desired degree of approximation to the optimal policy). Our algorithm thus provides a different complexity trade-off than classical algorithms such as value iteration---rather than scaling linearly in both horizon time and state space size, our running time trades an exponential dependence on the former in exchange for no dependence on the latter.",
issn="1573-0565",
doi="10.1023/A:1017932429737",
url="https://doi.org/10.1023/A:1017932429737"
}

@inproceedings{Kocsis2006,
 author = {Kocsis, Levente and Szepesv\'{a}ri, Csaba},
 title = {Bandit Based Monte-carlo Planning},
 booktitle = {Proceedings of the 17th European Conference on Machine Learning},
 series = {ECML'06},
 year = {2006},
 isbn = {3-540-45375-X, 978-3-540-45375-8},
 location = {Berlin, Germany},
 pages = {282--293},
 numpages = {12},
 url = {http://dx.doi.org/10.1007/11871842_29},
 doi = {10.1007/11871842_29},
 acmid = {2091633},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
}

@article{robbins1952,
author = "Robbins, Herbert",
fjournal = "Bulletin of the American Mathematical Society",
journal = "Bull. Amer. Math. Soc.",
month = "09",
number = "5",
pages = "527--535",
publisher = "American Mathematical Society",
title = "Some aspects of the sequential design of experiments",
url = "https://projecteuclid.org:443/euclid.bams/1183517370",
volume = "58",
year = "1952"
}

@article{Jaksch,
 author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
 title = {Near-optimal Regret Bounds for Reinforcement Learning},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2010},
 volume = {11},
 month = aug,
 year = {2010},
 issn = {1532-4435},
 pages = {1563--1600},
 numpages = {38},
 url = {http://dl.acm.org/citation.cfm?id=1756006.1859902},
 acmid = {1859902},
 publisher = {JMLR.org},
} 

@article{regretAnalysis,
url = {http://dx.doi.org/10.1561/2200000024},
year = {2012},
volume = {5},
journal = {Foundations and Trends® in Machine Learning},
title = {Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems},
doi = {10.1561/2200000024},
issn = {1935-8237},
number = {1},
pages = {1-122},
author = {Sébastien Bubeck and Nicolò Cesa-Bianchi}
}

@article{Gelly,
 author = {Gelly, Sylvain and Kocsis, Levente and Schoenauer, Marc and Sebag, Mich\`{e}le and Silver, David and Szepesv\'{a}ri, Csaba and Teytaud, Olivier},
 title = {The Grand Challenge of Computer Go: Monte Carlo Tree Search and Extensions},
 journal = {Commun. ACM},
 issue_date = {March 2012},
 volume = {55},
 number = {3},
 month = mar,
 year = {2012},
 issn = {0001-0782},
 pages = {106--113},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/2093548.2093574},
 doi = {10.1145/2093548.2093574},
 acmid = {2093574},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Silver2016,
  added-at = {2016-03-11T14:36:05.000+0100},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/29e987f58d895c490144693139cbc90c7/ytyoun},
  doi = {10.1038/nature16961},
  interhash = {48430c7891aaf9fe2582faa8f5d076c1},
  intrahash = {9e987f58d895c490144693139cbc90c7},
  journal = {Nature},
  keywords = {baduk go google},
  month = jan,
  number = 7587,
  pages = {484--489},
  publisher = {Nature Publishing Group},
  timestamp = {2016-03-11T14:37:40.000+0100},
  title = {Mastering the Game of {Go} with Deep Neural Networks and Tree Search},
  volume = 529,
  year = 2016
}

@article{IPP,
    author  = "Joseph Ledford",
    title   = "Partially Observable Belief MCTS to Play Cluedo",
    year    = "2018"
}

@ARTICLE{Cowling,
author = {P. I. Cowling and E. J. Powley and D. Whitehouse},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
title = {Information Set Monte Carlo Tree Search},
year = {2012},
volume = {4},
number = {2},
pages = {120-143},
keywords={Games;Monte Carlo methods;Algorithm design and analysis;Artificial intelligence;Uncertainty;Decision making;Computational modeling},
doi = {10.1109/TCIAIG.2012.2200894},
url = {doi.ieeecomputersociety.org/10.1109/TCIAIG.2012.2200894},
ISSN = {1943-068X},
month={June}
}

@book{Sutton-barto,
 author = {Sutton, Richard S. and Barto, Andrew G.},
 title = {Introduction to Reinforcement Learning},
 year = {1998},
 isbn = {0262193981},
 edition = {1st},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{Kaebling-Lozano,
author = {Leslie Pack Kaelbling and Tomas Lozano-Perez},
title = {Integrated Task and Motion Planning in Belief Space},
journal={International Journal of Robotics Research},
volume={32}, 
number={9-10},
year={2013},
keywords={Integrated Task and Motion Planning},
url={http://lis.csail.mit.edu/pubs/tlp/IJRRBelFinal.pdf}}

@book{Russell-norvig,
 author = {Russell, Stuart and Norvig, Peter},
 title = {Artificial Intelligence: A Modern Approach},
 year = {2009},
 isbn = {0136042597, 9780136042594},
 edition = {3rd},
 publisher = {Prentice Hall Press},
 address = {Upper Saddle River, NJ, USA},
}

@article{Savage,
author = {Leonard J. Savage},
year = {1954},
title = {The foundations of statistics. By Leonard J. Savage, John Wiley \& Sons, Inc., 1954, 294 pp},
journal = {Naval Research Logistics Quarterly},
volume = {1},
number = {3},
pages = {236-236},
doi = {10.1002/nav.3800010316},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800010316},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800010316}
}

@phdthesis{Mihai,
  author       = {Mihai Dobre}, 
  title        = {Low-resource Learning in Complex Games},
  school       = {University of Edinburgh},
  year         = 2018,
}